<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Bhagya Moharana ‚Äî AI/ML ¬∑ UX Research ¬∑ VR Prototyping</title>
  <meta name="description" content="Bhagya (Shoma) Moharana ‚Äî AI/ML, UX Research (physiological synchrony), and VR Prototyping. Unity teaching: Data Structures & Algorithms, Multiplayer with UNet, and Computer Illustrated Graphics.">
  <link rel="icon" type="image/png" href="shoma.jpg">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700;800&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css" />

  <!-- AOS (Animate On Scroll) CSS -->
  <link href="https://unpkg.com/aos@2.3.4/dist/aos.css" rel="stylesheet"/>
</head>
<body>

  <!-- NAVBAR -->
  <nav class="navbar">
    <div class="nav-container">
      <a class="brand" href="#">Bhagya Moharana</a>
      <ul class="nav-links">
        <li><a href="#aiml">AI/ML</a></li>
        <li><a href="#ux">UX Research</a></li>
        <li><a href="#vr">VR</a></li>
        <li><a href="#teaching">Teaching</a></li>
        <li><a href="#contact" class="pill">Contact</a></li>
      </ul>
    </div>
  </nav>


<!-- Hero Section -->
<section class="hero compact">
  <canvas id="hero-bg"></canvas>

  <div class="hero-container">
    <img src="shoma.jpg" alt="Dr. Bhagyabati Moharana" class="hero-photo">

    <div class="hero-content">
      <h1>Dr. Bhagyabati Moharana</h1>
      <h3>Machine Learning ¬∑ Statistical Modelling ¬∑ UX Research</h3>
      <p>
        PhD in <strong>Human-Computer Interaction</strong>, specializing in 
        <strong>Collaborative Virtual Reality (VR)</strong>, 
        <strong>Physiological Synchrony</strong>, and 
        <strong>Emotion-Aware AI Systems</strong>.  
        Bridging <strong>machine learning</strong>, <strong>UX analytics</strong>, and 
        <strong>multimodal signals</strong> for human-AI collaboration.
      </p>

      <div class="cta">
        <a href="#aiml" class="btn primary">Explore My Work</a>
        <a href="https://scholar.google.com/citations?hl=en&user=IkYXDjcAAAAJ" 
           target="_blank" class="btn outline">Google Scholar</a>
      </div>
    </div>
  </div>
</section>

<!-- üß† AI, Machine Learning & Human Data Fusion -->
<section id="aiml" class="section">
  <div class="section-head" data-aos="fade-up">
    <h2>üß† AI, Machine Learning & Human Data Fusion</h2>
    <p>
      I design intelligent systems that combine <strong>AI modeling</strong> with <strong>human-centric data</strong> ‚Äî
      integrating physiological (HR, EDA, pupil dilation), linguistic (NLP & AssemblyAI), and behavioral signals 
      to uncover patterns of emotion, workload, and collaboration.  
      Using <strong>PyTorch</strong>, <strong>CUDA</strong>, <strong>Python</strong>, <strong>R</strong>, and 
      <strong>Scikit-learn</strong>, I build interpretable multimodal pipelines that advance both 
      <strong>scientific understanding</strong> and <strong>real-world AI applications</strong>.
    </p>
  </div>

  <div class="chips" data-aos="zoom-in" data-aos-delay="100">
    <span class="chip">PyTorch</span>
    <span class="chip">CUDA</span>
    <span class="chip">Hugging Face</span>
    <span class="chip">AssemblyAI</span>
    <span class="chip">Scikit-learn</span>
    <span class="chip">Python</span>
    <span class="chip">R</span>
    <span class="chip">NLP / LLMs</span>
    <span class="chip">Signal Processing</span>
    <span class="chip">Multimodal Fusion</span>
  </div>

  <div class="row3" data-aos="fade-up" data-aos-delay="150">
    <article class="card">
      <h3>‚öôÔ∏è Model Optimization & Experimentation</h3>
      <p>
        Developed and fine-tuned deep learning models using <strong>PyTorch</strong> and <strong>CUDA</strong> 
        for GPU-accelerated training and inference.  
        Applied <strong>cross-validation</strong>, <strong>grid search</strong>, and performance 
        benchmarking to ensure robust and reproducible results.
      </p>
    </article>

    <article class="card">
      <h3>üß© Multimodal Learning & Representation</h3>
      <p>
        Combined physiological (EDA, HR), vocal sentiment (AssemblyAI), and linguistic features 
        for predictive modeling of user experience.  
        Applied <strong>PCA</strong>, <strong>t-SNE</strong>, and <strong>clustering</strong> 
        to discover latent synchrony patterns across human data streams.
      </p>
    </article>

    <article class="card">
      <h3>üìä Visualization & Interpretability</h3>
      <p>
        Created <strong>interactive visual dashboards</strong> in <strong>Plotly</strong> and <strong>D3.js</strong> 
        for exploring neural attention, synchrony trends, and sentiment trajectories.  
        Implemented <strong>SHAP</strong> and <strong>Grad-CAM</strong> for explainable model insights.
      </p>
    </article>
  </div>

  <div class="plot-grid" data-aos="fade-up" data-aos-delay="250">
    <iframe src="pca_model_summary.html" frameborder="0"></iframe>
    <iframe src="cluster_evaluation_summary.html" frameborder="0"></iframe>
    <iframe src="timeseries_significance.html" frameborder="0"></iframe>
    <iframe src="xr_twitch.html" frameborder="0"></iframe>
  </div>
</section>



 <section id="uxresearch" class="section alt">
  <div class="section-head">
    <h2>UX Research & Usability Evaluation</h2>
    <p>Bridging physiology and usability to design human-centered XR experiences.</p>
  </div>

  <p>I conduct <strong>mixed-method UX research</strong> integrating behavioral data, physiological signals, and subjective feedback to understand <strong>how humans collaborate and experience workload in immersive systems</strong>. My approach combines <strong>quantitative modeling</strong> with <strong>qualitative interpretation</strong>, ensuring that research outcomes directly inform <strong>design and product decisions</strong>.</p>

  <!-- Accordion: Study 1 -->
  <details class="ux-accordion">
    <summary>Study 1 ‚Äî Pilot (22 participants, 11 pairs)</summary>
    <ul>
      <li><strong>Objective:</strong> Identify how task roles influence perceived workload and physiological stress.</li>
      <li><strong>Methodology:</strong> Participants collaborated in VR using <strong>HTC Vive Pro</strong> (eye tracking) and <strong>Empatica E4</strong> wrist sensors (EDA, HR, IBI, ACC). Collected quantitative data (physiological + performance) and qualitative feedback (NASA-TLX, custom questionnaire).</li>
      <li><strong>Findings:</strong> Leaders showed higher stress and cognitive load; followers higher physical effort. <strong>Synchrony in HR and pupil dilation</strong> correlated with better performance and engagement. Introduced <strong>sentiment analysis</strong> for emotional alignment.</li>
      <li><strong>Outcome:</strong> Published in <em>IEEE VR 2023 ‚Äì Physiological Synchrony in Collaborative VR</em>.</li>
    </ul>
    <div class="research-images">
      <figure>
        <img src="uxr_study1_setup.png" alt="Study 1: Collaborative VR setup with Empatica E4 and HTC Vive Pro sensors">
        <figcaption>Study 1 ‚Äî Pilot (22 participants, 11 pairs): In-lab collaborative VR setup using HTC Vive Pro and Empatica E4 for physiological data collection.</figcaption>
      </figure>
    </div>
  </details>

  <!-- Accordion: Study 2 -->
  <details class="ux-accordion">
    <summary>Study 2 ‚Äî Full Experiment (68 participants, 34 pairs)</summary>
    <ul>
      <li><strong>Objective:</strong> Examine how <strong>open vs. closed spatial visibility</strong> influences team synchrony and user experience.</li>
      <li><strong>Methodology:</strong> Two-lab immersive study connecting participants remotely via VR; <strong>HMD + Empatica E4</strong> data; mixed methods (SiM-TLX, VRNQ, Interest Questionnaire, conversation logs). Obtained ethics approval and managed all logistics independently.</li>
      <li><strong>Findings:</strong> Visibility and familiarity improved synchrony and collaboration. <strong>Fusion of physiological synchrony and positive sentiment</strong> predicted better teamwork and lower cognitive load.</li>
      <li><strong>Outcome:</strong> Published in <em>IEEE VR 2024 ‚Äì Role-Specific Responses ¬∑ Voices Unveiled</em>.</li>
    </ul>
    <div class="research-images">
      <figure>
        <img src="uxr_study2_remote.png" alt="Study 2: Remote collaboration between two labs using VR headsets">
        <figcaption>Study 2 ‚Äî Full Study (68 participants): Two labs connected via immersive VR application to analyze visibility effects on collaboration quality and UX.</figcaption>
      </figure>
    </div>
  </details>

  <!-- Key Impact -->
  <h3>Key Impact</h3>
  <ul>
    <li>Designed and executed <strong>IRB-approved human studies</strong> with Empatica physiological sensors, HMD-based eye tracking, and sentiment analysis.</li>
    <li>Delivered <strong>data-driven UX recommendations</strong> for visibility design, task clarity, and workload management.</li>
    <li>Developed <strong>multimodal analysis pipelines</strong> combining behavioral, physiological, and emotional data to guide product design.</li>
  </ul>
</section>


<!-- VR Section -->
<section id="vr" class="section alt">
  <div class="section-head" data-aos="fade-up">
    <h2>VR Prototyping (Unity)</h2>
    <p>Collaborative VR environments for studying team dynamics and UX in real time ‚Äî single & multiplayer (UNet), XR interaction, data capture.</p>
  </div>

  <div class="video-grid">
    <div class="v-card">
      <div class="frame">
        <iframe src="https://drive.google.com/file/d/104fUNe797jsEhIG1RATNRFeoa5Q5NDqf/preview" allow="autoplay" allowfullscreen></iframe>
      </div>
      <h4>Closed Room Collaboration</h4>
      <p>Audio-driven coordination; ideal for studying physiological synchrony.</p>
    </div>

    <div class="v-card">
      <div class="frame">
        <iframe src="https://drive.google.com/file/d/1Qy9uA12TT7MHtm9zCW3GsCtuCUcc5wa6/preview" allow="autoplay" allowfullscreen></iframe>
      </div>
      <h4>Open Room Collaboration</h4>
      <p>Avatar visibility aids spatial awareness and shared task understanding.</p>
    </div>

    <div class="v-card">
      <div class="frame">
        <iframe src="https://drive.google.com/file/d/1KqkN2QEKwFGA4cGrFGHMoUPNWOnjBKUS/preview" allow="autoplay" allowfullscreen></iframe>
      </div>
      <h4>Starter Multiplayer Unity Networking</h4>
      <p>Basic multiplayer setup with synchronized object interaction.</p>
    </div>

    <div class="v-card">
      <div class="frame">
        <iframe src="https://drive.google.com/file/d/1b7s-JpmjATFKJKZFU_g50QSKmYaqEZeK/preview" allow="autoplay" allowfullscreen></iframe>
      </div>
      <h4>Lift System in Closed Room</h4>
      <p>Illustrates dynamic object passing in VR collaborative tasks.</p>
    </div>

    <div class="v-card">
      <div class="frame">
        <iframe src="https://drive.google.com/file/d/17_oRX_Ins62dS1OHJScrv7tUbbmUWx6S/preview" allow="autoplay" allowfullscreen></iframe>
      </div>
      <h4>Robot Human Collaboration</h4>
      <p>Demonstrates human-robot interaction in VR collaborative setups.</p>
    </div>
  
   <div class="v-card">
      <div class="frame">
        <iframe src="https://drive.google.com/file/d/1N2e-6Ufb0k3o46tJKBzxsM_wX6ilAcRh/preview" allow="autoplay" allowfullscreen></iframe>
      </div>
      <h4>Keyboard, Avatar and Snap Zone</h4>
      <p>Demonstrates human-robot interaction in VR collaborative setups.</p>
    </div>
  </div>
</section>

 <!-- Research Publications -->
<section id="publications" class="section">
  <div class="section-head" data-aos="fade-up">
    <h2>üìÑ Research Publications</h2>
    <p>Peer-reviewed papers on physiological synchrony, XR collaboration, AI/ML, and multimodal UX analytics ‚Äî published across IEEE, Academia.edu, and ResearchGate.</p>
    <p class="hint">(Click on each year below to expand and view publications)</p>
  </div>

  <div class="pub-list" data-aos="fade-up" data-aos-delay="100">

    <!-- 2025 -->
    <details class="pub-year" open>
      <summary>üìò 2025</summary>
      <article class="pub-item">
        <h3>Visualizing Sentiment Dynamics in Collaborative VR: A Comparison of NLP Models</h3>
        <p><em>Bhagya Moharana</em>. <em>Under Review</em>, 2025.</p>
        <a href="#" target="_blank">Preprint (coming soon) ‚Üí</a>
      </article>
    </details>

    <!-- 2024 -->
    <details class="pub-year">
      <summary>üìó 2024</summary>

      <article class="pub-item">
        <h3>Role-Specific Physiological Responses and Quality of Experience in Collaborative Virtual Reality Tasks: A Comparative Study of Leaders and Followers</h3>
        <p><em>Bhagya Moharana</em>. <em>10th International Conference</em>, 2024.</p>
        <a href="https://ieeexplore.ieee.org/abstract/document/10868723/" target="_blank">IEEE Xplore ‚Üí</a>
      </article>

      <article class="pub-item">
        <h3>Voices Unveiled: Quality of Experience in Collaborative VR via AssemblyAI and NASA-TLX Analysis</h3>
        <p><em>Bhagya Moharana</em>. <em>16th International Conference</em>, 2024.</p>
        <a href="https://ieeexplore.ieee.org/abstract/document/10598261/" target="_blank">IEEE Xplore ‚Üí</a>
      </article>

      <article class="pub-item">
        <h3>Physiological Synchrony: A Novel Approach to Evaluating User Quality of Experience in Collaborative Distributed Virtual Reality Environments</h3>
        <p><em>Bhagya Moharana</em>. <em>Academia.edu</em>, 2024.</p>
        <a href="https://www.academia.edu/115620269/Physiological_Synchrony_A_Novel_Approach_to_Evaluating_User_Quality_of_Experience_in_Collaborative_Distributed_Virtual_Reality_Environments" target="_blank">Academia.edu ‚Üí</a>
      </article>

      <article class="pub-item">
        <h3>Exploring the Role of Pupil Dilation and Blink Rate in Perceived Workload in Collaborative Virtual Reality</h3>
        <p><em>Bhagya Moharana</em>. <em>IEEE Conference</em>, 2024.</p>
        <a href="https://ieeexplore.ieee.org/document/10598261" target="_blank">IEEE Xplore ‚Üí</a>
      </article>
    </details>

    <!-- 2023 -->
    <details class="pub-year">
      <summary>üìô 2023</summary>
      <article class="pub-item">
        <h3>Physiological Synchrony in a Collaborative Virtual Reality Task</h3>
        <p><em>Bhagya Moharana</em>. <em>15th International Conference</em>, 2023.</p>
        <a href="https://ieeexplore.ieee.org/abstract/document/10178661/" target="_blank">IEEE Xplore ‚Üí</a>
      </article>
    </details>

    <!-- 2022 -->
    <details class="pub-year">
      <summary>üìï 2022</summary>
      <article class="pub-item">
        <h3>Subjective Evaluation of Group User QoE in Collaborative Virtual Environment (CVE)</h3>
        <p><em>Bhagya Moharana</em>. <em>Proceedings of the 14th International Workshop</em>, 2022.</p>
        <a href="https://www.academia.edu/92788729/Subjective_Evaluation_of_Group_User_QoE_in_Collaborative_Virtual_Environment_CVE" target="_blank">Academia.edu ‚Üí</a>
      </article>

      <article class="pub-item">
        <h3>Understanding Group Quality of Experience of Immersive Virtual Reality Collaborative Design Applications</h3>
        <p><em>Bhagya Moharana</em>. <em>ResearchGate</em>, 2022.</p>
        <a href="https://www.researchgate.net/publication/364345186_Understanding_Group_Quality_of_Experience_of_Immersive_Virtual_Reality_Collaborative_Design_Applications" target="_blank">ResearchGate ‚Üí</a>
      </article>

      <article class="pub-item">
        <h3>A Quality of Experience Evaluation of Collaborative Design Tasks in Virtual Reality</h3>
        <p><em>Bhagya Moharana</em>. <em>Academia.edu</em>, 2022.</p>
        <a href="https://www.academia.edu/92788720/A_Quality_of_Experience_Evaluation_of_Collaborative_Design_Tasks_in_Virtual_Reality" target="_blank">Academia.edu ‚Üí</a>
      </article>
    </details>

  </div>
</section>


  <!-- Teaching -->
  <section id="teaching" class="section alt">
    <div class="section-head" data-aos="fade-up">
      <h2>üéì Teaching</h2>
      <p>Lecturer & instructor with hands-on modules in Unity C#, Data Structures & Algorithms, Multiplayer with UNet, and Computer Illustrated Graphics.</p>
    </div>

    <div class="row3" data-aos="fade-up" data-aos-delay="120">
      <article class="card">
        <h3>Pulse College</h3>
        <p>Data Structures & Algorithms in Unity C#, designing single-player and multiplayer (UNet) systems; project-based learning.</p>
      </article>
      <article class="card">
        <h3>LOETB</h3>
        <p>Computer Illustrated Graphics: Adobe Illustrator, design foundations, and portfolio-ready creative work.</p>
      </article>
      <article class="card">
        <h3>Mentorship</h3>
        <p>Unity development, multimodal analysis, research methods, and scientific writing.</p>
      </article>
    </div>
  </section>

<section id="highlights" class="section alt">
  <div class="section-head" data-aos="fade-up">
    <h2>üåü Personal Highlights</h2>
    <p>Mentorships, talks, and achievements that shaped my research journey.</p>
  </div>

  <div class="ticker">
    <div class="ticker-track">
      <div class="ticker-item" data-description="üöÄ NASA Space Apps Challenge ‚Äî Mentor">
        <img src="nasa1.jpg" alt="NASA Event 1">
      </div>
      <div class="ticker-item" data-description="üöÄ NASA Space Apps Challenge ‚Äî Mentor">
        <img src="nasa2.jpg" alt="NASA Event 2">
      </div>

      <div class="ticker-item" data-description="üéì PhD Viva Defence ‚Äî Celebrating the milestone">
        <img src="phd1.jpg" alt="PhD Viva 1">
      </div>
      <div class="ticker-item" data-description="üéì PhD Viva Defence ‚Äî Celebrating the milestone">
        <img src="phd2.jpg" alt="PhD Viva 2">
      </div>

      <div class="ticker-item" data-description="üöÄ Student Inc Entrepreneurship Program">
        <img src="studentinc1.jpg" alt="Student Inc 1">
      </div>
      <div class="ticker-item" data-description="üöÄ Student Inc Entrepreneurship Program">
        <img src="studentinc2.jpg" alt="Student Inc 2">
      </div>

      <div class="ticker-item" data-description="üíº Google & Scale Ireland Entrepreneurship Meetup">
        <img src="G1.jpg" alt="Google Event 1">
      </div>
      <div class="ticker-item" data-description="üíº Qomex Conference Research Paper Presentation at Ghent">
        <img src="Qomex.PNG" alt="Google Event 2">
      </div>

      <div class="ticker-item" data-description="Immersive Lab Athlone">
        <img src="L1.PNG" alt="Social XR Event">
      </div>
      <div class="ticker-item" data-description="Immersive Lab Athlone">
        <img src="L2.PNG" alt="Social XR Event">
      </div>
       <div class="ticker-item" data-description="Immersive Lab Athlone">
        <img src="L3.PNG" alt="Social XR Event">
      </div>
    </div>
  </div>

  <!-- Lightbox -->
  <div id="lightbox" class="lightbox">
    <span class="close">&times;</span>
    <img class="lightbox-img" src="" alt="">
    <p class="lightbox-desc"></p>
  </div>
</section>


<!-- Contact -->
<section id="contact" class="section contact">
  <div class="section-head" data-aos="fade-up">
    <h2>Contact</h2>
    <p>Let's connect</p>
  </div>

  <div class="social-icons" data-aos="zoom-in" data-aos-delay="100">
    <a href="https://x.com/BhagyaMoharana" target="_blank" aria-label="X (Twitter)">
      <img src="x.png" alt="X logo">
    </a>
    <a href="https://github.com/bhagyamoharana" target="_blank" aria-label="GitHub">
      <img src="git.png" alt="GitHub logo">
    </a>
    <a href="https://www.linkedin.com/in/bhagyabati-m-a07619172/" target="_blank" aria-label="LinkedIn">
      <img src="link.png" alt="LinkedIn logo">
    </a>
    <a href="https://scholar.google.com/citations?hl=en&user=IkYXDjcAAAAJ" target="_blank" aria-label="Google Scholar">
      <img src="Goo.png" alt="Google Scholar logo">
    </a>
    <a href="mailto:b.moharana@research.ait.ie" aria-label="Email">
      <img src="gm.png" alt="Gmail logo">
    </a>
  </div>
</section>



  <footer class="footer">
    <p>¬© 2025 Bhagya Moharana</p>
  </footer>

  <!-- AOS JS -->
  <script src="https://unpkg.com/aos@2.3.4/dist/aos.js"></script>
  <script>
    AOS.init({
      once: true,
      duration: 700,
      easing: 'ease-out',
      offset: 80,
    });
  </script>
<script>
  const lightbox = document.getElementById('lightbox');
  const lightboxImg = document.querySelector('.lightbox-img');
  const lightboxDesc = document.querySelector('.lightbox-desc');
  const closeBtn = document.querySelector('.lightbox .close');

  document.querySelectorAll('.ticker-item').forEach(item => {
    item.addEventListener('click', () => {
      lightboxImg.src = item.querySelector('img').src;
      lightboxDesc.textContent = item.dataset.description;
      lightbox.style.display = 'flex';
    });
  });

  closeBtn.addEventListener('click', () => {
    lightbox.style.display = 'none';
  });

  lightbox.addEventListener('click', (e) => {
    if (e.target === lightbox) {
      lightbox.style.display = 'none';
    }
  });
</script>




</body>
</html>
